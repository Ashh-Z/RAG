{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "import requests \n",
    "import fitz\n",
    "from tqdm import tqdm \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'visual para.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str : \n",
    "    clean_txt = text.replace(\"\\n\",\" \").strip()\n",
    "    return clean_txt\n",
    "\n",
    "\n",
    "#     return pages_and_texts\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    print(len(doc))\n",
    "    n = len(doc)\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        if page_number <= n :\n",
    "            text = page.get_text()  # get plain text encoded as UTF-8\n",
    "            text = text_formatter(text)\n",
    "            pages_and_texts.append({\"page_number\": page_number,  \n",
    "                                    \"page_char_count\": len(text),\n",
    "                                    \"page_word_count\": len(text.split(\" \")),\n",
    "                                    \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                    \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                    \"text\": text})\n",
    "    return pages_and_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.sample(pages_and_texts, k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", trust_remote_code=True)\n",
    "# In case you want to reduce the maximum length:\n",
    "model.max_seq_length = 8192\n",
    "\n",
    "queries = [\n",
    "    \"how much protein should a female eat\",\n",
    "    \"summit define\",\n",
    "]\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\n",
    "]\n",
    "\n",
    "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "scores = (query_embeddings @ document_embeddings.T) * 100\n",
    "print(scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# from torch import Tensor\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# def last_token_pool(last_hidden_states: Tensor,\n",
    "#                  attention_mask: Tensor) -> Tensor:\n",
    "#     left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "#     if left_padding:\n",
    "#         return last_hidden_states[:, -1]\n",
    "#     else:\n",
    "#         sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "#         batch_size = last_hidden_states.shape[0]\n",
    "#         return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "# def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "#     return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "\n",
    "# # Each query must come with a one-sentence instruction that describes the task\n",
    "# task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "# queries = [\n",
    "#     get_detailed_instruct(task, 'how much protein should a female eat'),\n",
    "#     get_detailed_instruct(task, 'summit define')\n",
    "# ]\n",
    "# # No need to add instruction for retrieval documents\n",
    "# documents = [\n",
    "#     \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "#     \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n",
    "# ]\n",
    "# input_texts = queries + documents\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-Qwen2-1.5B-instruct', trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained('Alibaba-NLP/gte-Qwen2-1.5B-instruct', trust_remote_code=True)\n",
    "\n",
    "# max_length = 8192\n",
    "\n",
    "# # Tokenize the input texts\n",
    "# batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt')\n",
    "# outputs = model(**batch_dict)\n",
    "# embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "# # normalize embeddings\n",
    "# embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "# scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "# print(scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English \n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "doc = nlp(\"Hi, I saw you standing there. What were you doing?\")\n",
    "\n",
    "# assert(len(list(doc.sents))) == 3\n",
    "\n",
    "s = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "txt = \"Hi, I saw you standing there. What were you doing?\"\n",
    "l = nltk.tokenize.sent_tokenize(txt, language='english')\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(pages_and_texts) : \n",
    "    text = item['text']\n",
    "    item[\"sentences\"] = nltk.tokenize.sent_tokenize(text, language='english') \n",
    "\n",
    "    item['page_sentence_count_nltk'] = len(item['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We will chunk sentences into groups of 5 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10\n",
    "\n",
    "def chunking(input_list , chunk_size) :\n",
    "    l = [input_list[i : i+ chunk_size] for i in range(0,len(input_list), chunk_size)]\n",
    "    return l \n",
    "\n",
    "test = list(range(21))\n",
    "\n",
    "chunking(test,chunk_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(pages_and_texts) : \n",
    "    item[\"chunks\"] = chunking(item['sentences'], chunk_size)\n",
    "    item['num_chunks'] = len(item[\"chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts[6]['chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts[6]['num_chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for chunk in item[\"chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"chunks\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts[6]['chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_texts[6]['num_chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks, k = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering very short chunks, they may not contain much info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Removing very short chunks \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_token_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"chunk_token_count\"] <= min_token_len][\"chunks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df[df[\"chunk_token_count\"] <= min_token_len].sample(2).iterrows(): \n",
    "    print(f'CHunk token count : {row[1][\"chunk_token_count\"]} | text : {row[1][\"chunks\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks_over_threshold = df[df[\"chunk_token_count\"] > min_token_len].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_threshold[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks_over_threshold, k =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", trust_remote_code=True)\n",
    "# In case you want to reduce the maximum length:\n",
    "model.max_seq_length = 8192\n",
    "\n",
    "queries = [\n",
    "    \"how much protein should a female eat\",\n",
    "    \"summit define\",\n",
    "]\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\",\n",
    "]\n",
    "\n",
    "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "scores = (query_embeddings @ document_embeddings.T) * 100\n",
    "print(scores.tolist())\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing \"\"\"\n",
    "\n",
    "test_sentences = [\"Testing a local rag system.\", \" I hope this works.\", \"I am wasting too much time on this.\"]\n",
    "\n",
    "test_embeddings = model.encode(test_sentences)\n",
    "embeddings_dict = dict(zip(test_sentences, test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the embeddings\n",
    "for test_sentences, test_sentences in embeddings_dict.items():\n",
    "    print(\"Sentence:\", test_sentences)\n",
    "    print(\"Embedding:\", test_embeddings)\n",
    "    print(\"Embedding size :\", test_embeddings.shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = [item[\"chunks\"] for item in pages_and_chunks_over_threshold]\n",
    "text_chunks[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings one by one on the GPU\n",
    "for item in tqdm(pages_and_chunks_over_threshold):\n",
    "    item[\"embedding\"] = model.encode(item[\"chunks\"], batch_size=32, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunk_embeddings = model.encode(text_chunks,\n",
    "#                                                batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n",
    "#                                                convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
    "\n",
    "# text_chunk_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks_over_threshold[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunk_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk_embeddings_df = pd.DataFrame(pages_and_chunks_over_threshold)\n",
    "embedding_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunk_embeddings_df.to_csv(embedding_df_save_path, index=False, escapechar='\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import saved file and view\n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embedding_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming pages_and_chunks_over_threshold is a list of dictionaries and 'embedding' is one of the keys\n",
    "text_chunk_embeddings_df = pd.DataFrame(pages_and_chunks_over_threshold)\n",
    "\n",
    "# Save the entire DataFrame including embeddings using pickle\n",
    "text_chunk_embeddings_df.to_pickle(\"text_chunks_and_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire DataFrame including embeddings using pickle\n",
    "text_chunks_and_embedding_df = pd.read_pickle(\"text_chunks_and_embeddings.pkl\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device\n",
    "embeddings_tensor = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "print(embeddings_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG search and answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load DataFrame from pickle file\n",
    "text_chunks_and_embedding_df = pd.read_pickle(\"text_chunks_and_embeddings.pkl\")\n",
    "\n",
    "# Example: Convert back to torch tensor assuming 'embedding' is a key containing numpy arrays\n",
    "embeddings_tensor = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32)\n",
    "\n",
    "# Prepare a similar DataFrame for the loaded data\n",
    "loaded_df = pd.DataFrame(text_chunks_and_embedding_df)\n",
    "\n",
    "# Ensure 'embedding' column remains as numpy arrays\n",
    "loaded_df[\"embedding\"] = loaded_df[\"embedding\"].apply(lambda x: np.array(x))\n",
    "\n",
    "# Now you have a DataFrame 'loaded_df' which should be structurally similar to 'text_chunk_embeddings_df'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import random\n",
    "\n",
    "# # import torch\n",
    "# import numpy as np \n",
    "# import pandas as pd\n",
    "\n",
    "# # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Import texts and embedding df\n",
    "# text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# # Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "# # text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# # Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "# # text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\"  \"))\n",
    "\n",
    "# # embeddings = torch.tensor(np.stack(text_chunks_and_embedding_df[\"embedding\"].to_list(), axis=0))\n",
    "\n",
    "\n",
    "# embeddings_from_df = text_chunk_embeddings_df[\"embedding\"].tolist()\n",
    "# # embeddings_from_df\n",
    "\n",
    "# embeddings = torch.tensor(embeddings_from_df, dtype=torch.float32).to(device)\n",
    "\n",
    "# # # Convert texts and embedding df to list of dicts\n",
    "# # pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import ast\n",
    "# import torch\n",
    "\n",
    "# # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Import texts and embedding df\n",
    "# text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# def safe_literal_eval(val):\n",
    "#     try:\n",
    "#         return np.array(ast.literal_eval(val))\n",
    "#     except (SyntaxError, ValueError):\n",
    "#         # Handle cases where the string might not be properly formatted\n",
    "#         return np.array([])\n",
    "\n",
    "# # Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "# text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(safe_literal_eval)\n",
    "\n",
    "# # Check for and remove any empty arrays resulting from parsing errors\n",
    "# text_chunks_and_embedding_df = text_chunks_and_embedding_df[text_chunks_and_embedding_df[\"embedding\"].apply(len) > 0]\n",
    "\n",
    "# # Stack the numpy arrays into a single numpy array\n",
    "# embeddings = np.stack(text_chunks_and_embedding_df[\"embedding\"].to_list(), axis=0)\n",
    "\n",
    "# # Convert the numpy array to a torch tensor\n",
    "# embeddings = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "# # # Convert texts and embedding df to list of dicts\n",
    "# # pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = (embeddings_from_df).to(device)\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "# pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks_and_embedding_df[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query embedding and stored embedding matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor = embeddings_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ZoDiac Watermarking\"\n",
    "print(f\"query : {query}\")\n",
    "\n",
    "query_embeddings = model.encode(query, convert_to_tensor=True ).to(device)\n",
    "\n",
    "dot_scores = util.dot_score(a= query_embeddings, b=embeddings_tensor)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_dot_results = torch.topk(dot_scores,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_dot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text,wrap_length=80) : \n",
    "    wrapped_text = textwrap.fill(text,wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tree-Ring Watermarking\"\n",
    "print(f\"query : {query}\")\n",
    "\n",
    "for value, index in zip(top_k_dot_results[0], top_k_dot_results[1]): \n",
    "    print(f\"Score: {value:.4f}\")\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[index][\"chunks\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functinons for semantic search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(pages_and_chunks[index][\"chunks\"])\n",
    "        # Print the page number too so we can reference the textbook further and check the results\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the texts of the top scores\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/huggingface/transformers/blob/25245ec26dc29bcf6102e1b4ddd0dfd02e720cf5/src/transformers/generation/logits_process.py#L411\n",
    "# from transformers.generation.logits_process import LogitsWarper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TopPLogitsWarper(LogitsWarper):\n",
    "#     \"\"\"\n",
    "#     [`LogitsWarper`] that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <= prob_cut_off. Often\n",
    "#     used together with [`TemperatureLogitsWarper`] and [`TopKLogitsWarper`].\n",
    "\n",
    "#     Args:\n",
    "#         top_p (`float`):\n",
    "#             If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or\n",
    "#             higher are kept for generation.\n",
    "#         filter_value (`float`, *optional*, defaults to -inf):\n",
    "#             All filtered values will be set to this float value.\n",
    "#         min_tokens_to_keep (`int`, *optional*, defaults to 1):\n",
    "#             Minimum number of tokens that cannot be filtered.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, top_p: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
    "#         top_p = float(top_p)\n",
    "#         if top_p < 0 or top_p > 1.0:\n",
    "#             raise ValueError(f\"`top_p` has to be a float > 0 and < 1, but is {top_p}\")\n",
    "#         if not isinstance(min_tokens_to_keep, int) or (min_tokens_to_keep < 1):\n",
    "#             raise ValueError(f\"`min_tokens_to_keep` has to be a positive integer, but is {min_tokens_to_keep}\")\n",
    "\n",
    "#         self.top_p = top_p\n",
    "#         self.filter_value = filter_value\n",
    "#         self.min_tokens_to_keep = min_tokens_to_keep\n",
    "\n",
    "#     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "#         s_sorted_vals, s_sorted_indices = torch.sort(scores, descending=True, dim = -1)\n",
    "#         softmax_outputs_cumsum = s_sorted_vals.softmax(dim = -1, ).cumsum(dim = -1)\n",
    "#         indices_to_remove = softmax_outputs_cumsum <= self.top_p\n",
    "#         indices_to_remove = indices_to_remove.scatter(1, s_sorted_indices, indices_to_remove)\n",
    "#         indices_to_remove = ~indices_to_remove\n",
    "#         scores_processed = scores.masked_fill(indices_to_remove, self.filter_value)\n",
    "#         #print(scores[0], scores_processed[0], '11')\n",
    "#         return scores_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel, LogitsProcessorList\n",
    "# from transformers import MaxLengthCriteria, StoppingCriteriaList  # Correct import path\n",
    "# import torch\n",
    "\n",
    "# # Initialize the tokenizer and model\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # Setup the prompt and other beam search settings\n",
    "# x = 'The capital of India?'\n",
    "# input_ids = tokenizer(x, return_tensors='pt').input_ids.to(model.device)\n",
    "# print(f'input_ids = {input_ids}')\n",
    "# y = 'Delhi'\n",
    "# output_ids = tokenizer(y, return_tensors='pt').input_ids.to(model.device)\n",
    "# print(f'output_ids = {output_ids}')\n",
    "\n",
    "# # Number of beams\n",
    "# num_beams = 10\n",
    "\n",
    "# logits_top_p = TopPLogitsWarper(top_p=0.9)\n",
    "\n",
    "# # Logits processor and stopping criteria\n",
    "# logits_processor = LogitsProcessorList([logits_top_p])\n",
    "\n",
    "\n",
    "# #Processing logits\n",
    "# Temp_scale = 2\n",
    "# with torch.no_grad():\n",
    "#     logits = model(input_ids).logits[0]\n",
    "#     processed_logits = logits_top_p(input_ids = input_ids, scores = logits)\n",
    "#     probabilities = (processed_logits / Temp_scale).softmax(dim = -1)\n",
    "#     probabilities_final = probabilities[-1, :]\n",
    "#     print(probabilities.shape, torch.argmax(probabilities_final), probabilities_final[13856])\n",
    "#     #fx_y = probabilities_final[]\n",
    "\n",
    "\n",
    "# # Generate text using beam search\n",
    "# output_sequences = model.generate(\n",
    "#     input_ids,\n",
    "#     max_length=12,\n",
    "#     num_beams=num_beams,\n",
    "#     num_return_sequences=2,\n",
    "#     logits_processor=logits_processor,\n",
    "# )\n",
    "\n",
    "# # Decode and print the output beams\n",
    "# for index, output_sequence in enumerate(output_sequences):\n",
    "#     output_text = tokenizer.decode(output_sequence, skip_special_tokens=True)\n",
    "#     print(f'beam {index}: {output_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemma-2-9b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_flash_attn_2_available \n",
    "\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline \n",
    "\n",
    "# model_id  = \"google/gemma-2-9b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# import torch\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"google/gemma-2-9b-it\",\n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device=\"cuda\",\n",
    "# )\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n",
    "# ]\n",
    "# outputs = pipe(\n",
    "#     messages,\n",
    "#     max_new_tokens=1024,\n",
    "#     do_sample=False,\n",
    "# )\n",
    "# assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "# print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# llm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = llm.generate(**input_ids, max_new_tokens=512)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = \"Write a poem about time in 50 words\"\n",
    "# input_ids = tokenizer(input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(**input_ids)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = \"Write a poem about time\"\n",
    "# dialogue_template = [{\"role\": \"user\", \"content\": input_text}]\n",
    "\n",
    "# # Assuming input_data is a tensor, directly move it to the GPU\n",
    "# input_data = tokenizer.apply_chat_template(conversation=dialogue_template, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# # Generate outputs directly using input_data\n",
    "# outputs = model.generate(input_ids=input_data, max_new_tokens=256)\n",
    "\n",
    "# # Decode and print the output\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Write a poem about time\"\n",
    "dialogue_template = [{\"role\": \"user\", \"content\": input_text}]\n",
    "\n",
    "# Assuming input_data is a tensor, directly move it to the GPU\n",
    "prompt = tokenizer.apply_chat_template(dialogue_template, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate outputs directly using input_data\n",
    "outputs = llm.generate(**input_ids, max_new_tokens=256)\n",
    "\n",
    "# Decode and print the output\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prompt_formatter(query, context_items ):\n",
    "#     \"\"\"\n",
    "#     Augments query with text-based context from context_items.\n",
    "#     \"\"\"\n",
    "#     # Join context items into one dotted paragraph\n",
    "#     context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "#     # Create a base prompt with examples to help the model\n",
    "#     # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "#     # We could also write this in a txt file and import it in if we wanted.\n",
    "#     base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "# Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "# Don't return the thinking, only return the answer.\n",
    "# Make sure your answers are as explanatory as possible.\n",
    "# Use the following examples as reference for the ideal answer style.\n",
    "# \\nExample 1:\n",
    "# Query: What are the fat-soluble vitamins?\n",
    "# Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "# \\nExample 2:\n",
    "# Query: What are the causes of type 2 diabetes?\n",
    "# Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
    "# \\nExample 3:\n",
    "# Query: What is the importance of hydration for physical performance?\n",
    "# Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
    "# \\nNow use the following context items to answer the user query:\n",
    "# {context}\n",
    "# \\nRelevant passages: <extract relevant passages from the context here>\n",
    "# User query: {query}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "#     # Update base prompt with context items and query   \n",
    "#     base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "#     # Create prompt template for instruction-tuned model\n",
    "#     dialogue_template = [\n",
    "#         {\"role\": \"user\",\n",
    "#         \"content\": base_prompt}\n",
    "#     ]\n",
    "\n",
    "#     # Apply the chat template\n",
    "#     prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "#                                           tokenize=False,\n",
    "#                                           add_generation_prompt=True)\n",
    "#     return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query, context_items, use_dialogue_template=True):\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    # context = \"- \" + \"\\n- \".join([item[\"chunks\"] for item in context_items])\n",
    "    context = \" \".join([item[\"chunks\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"\n",
    "        Based on the following context items, please answer the query.\n",
    "        Context item : \n",
    "        {context}\n",
    "        User query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    if(use_dialogue_template == True) :\n",
    "        # Apply the chat template\n",
    "        prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True)\n",
    "    else : \n",
    "        prompt = tokenizer.apply_chat_template(conversation=base_prompt,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True) \n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain the black-box visual paraphrase\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get relevant resources\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "    \n",
    "# Create a list of context items\n",
    "context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "# Format prompt with context items\n",
    "prompt = prompt_formatter(query=query,\n",
    "                          context_items=context_items)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "outputs = llm.generate(**input_ids,\n",
    "                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
    "                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n",
    "                             max_new_tokens=256) # how many new tokens to generate from prompt \n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get just the scores and indices of top related results\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings)\n",
    "    \n",
    "    # Create a list of context items\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu() # return score back to CPU \n",
    "        \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = llm.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = random.choice(query_list)\n",
    "query = \"What is the aim of this paper?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer query with context and return context \n",
    "answer, context_items = ask(query=query, \n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)\n",
    "print(f\"Context items:\")\n",
    "context_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashhar_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
